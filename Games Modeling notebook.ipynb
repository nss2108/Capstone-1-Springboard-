{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "games=pd.read_csv(\"gamesdataset.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "games[\"averageplayers\"]=(games[\"maxplayers\"]-games[\"minplayers\"])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23932\n",
      "23930\n"
     ]
    }
   ],
   "source": [
    "train=games[[\"playingtime\", \"weight\", \"median\", \"numcomments\", \"owned\", \"trading\", \"usersrated\", \"wanting\", \"average\", \"geekscore\"]]\n",
    "print(len(train))\n",
    "train=train.dropna()\n",
    "print(len(train))\n",
    "X=train[[\"weight\", \"median\", \"owned\", \"trading\", \"wanting\", \"geekscore\"]]\n",
    "y=train[\"average\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.0\n",
       "1        2.5\n",
       "2        2.0\n",
       "3        2.5\n",
       "4        0.0\n",
       "5        1.5\n",
       "6        2.0\n",
       "7        1.0\n",
       "8        1.0\n",
       "9        1.5\n",
       "10       0.0\n",
       "11       0.5\n",
       "12       0.0\n",
       "13       0.0\n",
       "14       2.0\n",
       "15       1.0\n",
       "16       1.0\n",
       "17       1.0\n",
       "18       3.0\n",
       "19       0.0\n",
       "20       0.5\n",
       "21       0.0\n",
       "22       0.0\n",
       "23       1.0\n",
       "24       1.5\n",
       "25       2.0\n",
       "26       3.5\n",
       "27       2.5\n",
       "28       1.5\n",
       "29       2.0\n",
       "        ... \n",
       "23902    0.0\n",
       "23903    0.0\n",
       "23904    1.0\n",
       "23905    9.0\n",
       "23906    2.0\n",
       "23907    2.0\n",
       "23908    1.5\n",
       "23909    1.0\n",
       "23910    0.0\n",
       "23911    0.0\n",
       "23912    0.0\n",
       "23913    0.5\n",
       "23914    0.0\n",
       "23915    1.0\n",
       "23916    0.0\n",
       "23917    1.5\n",
       "23918    1.5\n",
       "23919    0.0\n",
       "23920    1.0\n",
       "23921    2.0\n",
       "23922    1.0\n",
       "23923    0.0\n",
       "23924    1.0\n",
       "23925    1.0\n",
       "23926    1.0\n",
       "23927    0.5\n",
       "23928    1.0\n",
       "23929    2.0\n",
       "23930    1.0\n",
       "23931    3.0\n",
       "Name: averageplayers, Length: 23932, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [  3.37746941e-01  -1.38593564e-15   4.04341656e-05  -3.34926031e-03\n",
      "   4.65113542e-03   2.28701638e-02]\n",
      "Mean squared error: 0.99\n",
      "Variance score: 0.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm=LinearRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "print('Coefficients: \\n', lm.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % np.mean((lm.predict(X_test) - Y_test) ** 2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % lm.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>average</td>     <th>  R-squared:         </th> <td>   0.767</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.767</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.577e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 30 Jul 2017</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:21:14</td>     <th>  Log-Likelihood:    </th> <td> -59675.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 23930</td>      <th>  AIC:               </th> <td>1.194e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 23925</td>      <th>  BIC:               </th> <td>1.194e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weight</th>    <td>    2.4467</td> <td>    0.014</td> <td>  175.049</td> <td> 0.000</td> <td>    2.419</td> <td>    2.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>median</th>    <td> -2.41e-14</td> <td> 1.39e-16</td> <td> -172.932</td> <td> 0.000</td> <td>-2.44e-14</td> <td>-2.38e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>owned</th>     <td>    0.0003</td> <td>  3.9e-05</td> <td>    7.150</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trading</th>   <td>   -0.0052</td> <td>    0.001</td> <td>   -5.762</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wanting</th>   <td>   -0.0035</td> <td>    0.001</td> <td>   -6.213</td> <td> 0.000</td> <td>   -0.005</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>geekscore</th> <td>    0.2595</td> <td>    0.008</td> <td>   34.405</td> <td> 0.000</td> <td>    0.245</td> <td>    0.274</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>313.277</td> <th>  Durbin-Watson:     </th> <td>   1.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 325.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.280</td>  <th>  Prob(JB):          </th> <td>1.75e-71</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.116</td>  <th>  Cond. No.          </th> <td>2.79e+17</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                average   R-squared:                       0.767\n",
       "Model:                            OLS   Adj. R-squared:                  0.767\n",
       "Method:                 Least Squares   F-statistic:                 1.577e+04\n",
       "Date:                Sun, 30 Jul 2017   Prob (F-statistic):               0.00\n",
       "Time:                        20:21:14   Log-Likelihood:                -59675.\n",
       "No. Observations:               23930   AIC:                         1.194e+05\n",
       "Df Residuals:                   23925   BIC:                         1.194e+05\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "weight         2.4467      0.014    175.049      0.000       2.419       2.474\n",
       "median      -2.41e-14   1.39e-16   -172.932      0.000   -2.44e-14   -2.38e-14\n",
       "owned          0.0003    3.9e-05      7.150      0.000       0.000       0.000\n",
       "trading       -0.0052      0.001     -5.762      0.000      -0.007      -0.003\n",
       "wanting       -0.0035      0.001     -6.213      0.000      -0.005      -0.002\n",
       "geekscore      0.2595      0.008     34.405      0.000       0.245       0.274\n",
       "==============================================================================\n",
       "Omnibus:                      313.277   Durbin-Watson:                   1.384\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              325.843\n",
       "Skew:                           0.280   Prob(JB):                     1.75e-71\n",
       "Kurtosis:                       3.116   Cond. No.                     2.79e+17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.93e-25. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "m = sm.OLS(y, X).fit()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>average</td>     <th>  R-squared:         </th> <td>   0.766</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.766</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.052e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 30 Jul 2017</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:21:14</td>     <th>  Log-Likelihood:    </th> <td> -39995.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 16033</td>      <th>  AIC:               </th> <td>8.000e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 16028</td>      <th>  BIC:               </th> <td>8.004e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weight</th>    <td>    2.4435</td> <td>    0.017</td> <td>  142.686</td> <td> 0.000</td> <td>    2.410</td> <td>    2.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>median</th>    <td> 8.203e-14</td> <td> 5.76e-16</td> <td>  142.349</td> <td> 0.000</td> <td> 8.09e-14</td> <td> 8.32e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>owned</th>     <td>    0.0003</td> <td> 4.55e-05</td> <td>    6.000</td> <td> 0.000</td> <td>    0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>trading</th>   <td>   -0.0052</td> <td>    0.001</td> <td>   -4.794</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>wanting</th>   <td>   -0.0035</td> <td>    0.001</td> <td>   -4.838</td> <td> 0.000</td> <td>   -0.005</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>geekscore</th> <td>    0.2629</td> <td>    0.009</td> <td>   28.437</td> <td> 0.000</td> <td>    0.245</td> <td>    0.281</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>187.904</td> <th>  Durbin-Watson:     </th> <td>   1.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 195.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.260</td>  <th>  Prob(JB):          </th> <td>3.93e-43</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.150</td>  <th>  Cond. No.          </th> <td>1.53e+18</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                average   R-squared:                       0.766\n",
       "Model:                            OLS   Adj. R-squared:                  0.766\n",
       "Method:                 Least Squares   F-statistic:                 1.052e+04\n",
       "Date:                Sun, 30 Jul 2017   Prob (F-statistic):               0.00\n",
       "Time:                        20:21:14   Log-Likelihood:                -39995.\n",
       "No. Observations:               16033   AIC:                         8.000e+04\n",
       "Df Residuals:                   16028   BIC:                         8.004e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "weight         2.4435      0.017    142.686      0.000       2.410       2.477\n",
       "median      8.203e-14   5.76e-16    142.349      0.000    8.09e-14    8.32e-14\n",
       "owned          0.0003   4.55e-05      6.000      0.000       0.000       0.000\n",
       "trading       -0.0052      0.001     -4.794      0.000      -0.007      -0.003\n",
       "wanting       -0.0035      0.001     -4.838      0.000      -0.005      -0.002\n",
       "geekscore      0.2629      0.009     28.437      0.000       0.245       0.281\n",
       "==============================================================================\n",
       "Omnibus:                      187.904   Durbin-Watson:                   1.532\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              195.285\n",
       "Skew:                           0.260   Prob(JB):                     3.93e-43\n",
       "Kurtosis:                       3.150   Cond. No.                     1.53e+18\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 6.82e-27. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = sm.OLS(Y_train, X_train).fit()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_scaled=preprocessing.scale(X)\n",
    "y_scaled=preprocessing.scale(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['playingtime', 'weight', 'median', 'numcomments', 'owned', 'trading',\n",
       "       'usersrated', 'wanting', 'average', 'geekscore', 'ratings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.dropna()\n",
    "ratings=[]\n",
    "y=train.average.mean()\n",
    "for i in train.average:\n",
    "    if i> y:\n",
    "        ratings.append(\"Good\")\n",
    "    if i == y or i <y:\n",
    "        ratings.append(\"Bad\")\n",
    "train[\"ratings\"]=ratings\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xlr=train[[ \"weight\", \"owned\", \"trading\",  \"wanting\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.712351663045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Xtrainlr, Xtestlr, ytrainlr, ytestlr = train_test_split(Xlr.values, \n",
    "                                              (train.ratings == \"Good\").values,random_state=5)\n",
    "clf=LogisticRegression()\n",
    "clf.fit(Xtrainlr, ytrainlr)\n",
    "print(accuracy_score(clf.predict(Xtestlr), ytestlr)) #seems promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score, after cross validation is 0.7086737188567852\n",
      "0.710178840047\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "score = cv_score(clf, Xtestlr, ytestlr)\n",
    "print(\"The accuracy score, after cross validation is {}\".format(score))\n",
    "print(accuracy_score(clf.predict(Xtestlr), ytestlr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Value of C is {'C': 1}\n",
      "0.711850242353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "clf=GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "clf.fit(Xtrainlr,ytrainlr)\n",
    "print(\"The Best Value of C is {}\".format(clf.best_params_))\n",
    "clf=LogisticRegression(C=0.1)\n",
    "clf.fit(Xtrainlr,ytrainlr)\n",
    "print(accuracy_score(clf.predict(Xtestlr), ytestlr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the Logistic Regression Model out on Geekscore to see if it is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['playingtime', 'weight', 'median', 'numcomments', 'owned', 'trading',\n",
       "       'usersrated', 'wanting', 'average', 'geekscore', 'ratings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings =[]\n",
    "traings=train[train.geekscore >0]\n",
    "y=traings.geekscore.mean()\n",
    "for i in traings.geekscore:\n",
    "    if i> y:\n",
    "        ratings.append(\"Good\")\n",
    "    if i == y or i <y:\n",
    "        ratings.append(\"Bad\")\n",
    "traings[\"ratings\"]=ratings\n",
    "traings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xgslr=traings[[ \"weight\", \"usersrated\", \"median\", \"playingtime\", \"owned\", \"trading\", \"wanting\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88649851632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Xtrainlrgs, Xtestlrgs, ytrainlrgs, ytestlrgs = train_test_split(Xgslr.values, \n",
    "                                              (traings.ratings == \"Good\").values,random_state=5)\n",
    "clf=LogisticRegression()\n",
    "clf.fit(Xtrainlrgs, ytrainlrgs)\n",
    "print(accuracy_score(clf.predict(Xtestlrgs), ytestlrgs)) #seems promising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score, after cross validation is 0.8924324881467738\n",
      "0.892433234421\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "score = cv_score(clf, Xtestlrgs, ytestlrgs)\n",
    "print(\"The accuracy score, after cross validation is {}\".format(score))\n",
    "print(accuracy_score(clf.predict(Xtestlrgs), ytestlrgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Value of C is {'C': 0.1}\n",
      "0.886127596439\n"
     ]
    }
   ],
   "source": [
    "clf=GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "clf.fit(Xtrainlrgs,ytrainlrgs)\n",
    "print(\"The Best Value of C is {}\".format(clf.best_params_))\n",
    "clf=LogisticRegression(C=0.1)\n",
    "clf.fit(Xtrainlrgs,ytrainlrgs)\n",
    "print(accuracy_score(clf.predict(Xtestlrgs), ytestlrgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings=[]\n",
    "games.columns\n",
    "td=games[[\"description\",\"category\", \"mechanic\", \"publisher\", \"average\", \"geekscore\"]]\n",
    "td=td.dropna()\n",
    "td.description=td.description.str.replace(\"\\d+\", \"\")\n",
    "y = td.average.mean()\n",
    "for i in td.average:\n",
    "    if i> y:\n",
    "        ratings.append(\"Good\")\n",
    "    if i == y or i <y:\n",
    "        ratings.append(\"Bad\")\n",
    "td[\"ratings\"]=ratings\n",
    "td[\"docs\"]=td[\"description\"]+\" \"+td[\"category\"]+\" \"+td[\"mechanic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def make_xy(td, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(td.docs)\n",
    "    X=X.tocsc()\n",
    "    y=(td.ratings==\"Good\").values.astype(np.int)\n",
    "    return X,y\n",
    "X,y=make_xy(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769769415759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68151815181518149"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnb_train, Xnb_test, ynb_train, ynb_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "clf=MultinomialNB()\n",
    "clf.fit(Xnb_train, ynb_train)\n",
    "print(clf.score(Xnb_train, ynb_train))\n",
    "clf.score(Xnb_test, ynb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 40)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD3CAYAAADmBxSSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJSlB/w+nifQr3grL2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20d0a94ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = list((X > 0).sum(0).tolist()[0]) #I don't 100% understand what I'm doing here\n",
    "c = pd.Series(df) #this is all the terms\n",
    "b = np.arange(c.sum()) #this is their frequency\n",
    "plt.hist(c,b, normed=1, histtype=\"step\", color='r', cumulative=True, linewidth=1.5)\n",
    "plt.xlim(-1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def cv_score(clf, X, y):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += clf.score(X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "_, itest = train_test_split(range(td.shape[0]), train_size=0.7)\n",
    "mask = np.zeros(td.shape[0], dtype=np.bool)\n",
    "mask[itest] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.67359735973597368, 0.001, 0.001),\n",
       " (0.68250825082508249, 0.001, 0.01),\n",
       " (0.68267326732673284, 0.1, 0.01),\n",
       " (0.69042904290429041, 1, 1),\n",
       " (0.69801980198019797, 5, 3),\n",
       " (0.69900990099009908, 5, 4),\n",
       " (0.69950495049504957, 5, 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "alphas = [.001,.01,.1, 1, 5, 10, 100]\n",
    "best_min_df = [.001, .01,1,2,3,4,5] # YOUR TURN: put your value of min_df here.\n",
    "scores_list=[]\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "results_list=[]\n",
    "best_alpha = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:\n",
    "    for df in best_min_df:\n",
    "        vectorizer = CountVectorizer(min_df=df)\n",
    "        Xthis, ythis = make_xy(td, vectorizer)\n",
    "        Xtrainthis = Xthis[mask]\n",
    "        ytrainthis = ythis[mask]\n",
    "        clf=MultinomialNB(alpha=alpha)\n",
    "        score=cv_score(clf,Xtrainthis, ytrainthis)\n",
    "        scores_list.append((score,alpha, df))\n",
    "for i in scores_list:\n",
    "    if i[0] > maxscore:\n",
    "        maxscore=i[0]\n",
    "        results_list.append(i)\n",
    "x=len(results_list)-1\n",
    "best_alpha=results_list[x][1]\n",
    "best_min_df=results_list[x][2]\n",
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.768152\n",
      "Accuracy on test data:     0.693662\n"
     ]
    }
   ],
   "source": [
    "stoplist=[\"iota\", \"tau\", \"alpha\", \"kappa\", \"sigma\", \"rho\", \"eta\", \"upsilon\", \"omicron\", \"delta\", \"gmt\", \"bc\", \"anno\", \"domini\", \"ocs\", \"gamma\", \"pi\"]\n",
    "vectorizer = CountVectorizer(min_df=best_min_df, stop_words=stoplist)\n",
    "X, y = make_xy(td, vectorizer)\n",
    "xtrain=X[mask]\n",
    "ytrain=y[mask]\n",
    "xtest=X[~mask]\n",
    "ytest=y[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\n",
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4515 2216]\n",
      " [2115 5292]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest, clf.predict(xtest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good words\t     P(good | word)\n",
      "                  mm 0.87\n",
      "            prussian 0.87\n",
      "              module 0.85\n",
      "                 aid 0.85\n",
      "            colonies 0.84\n",
      "            resource 0.83\n",
      "               glory 0.83\n",
      "               crops 0.83\n",
      "                tank 0.83\n",
      "            narrator 0.82\n",
      "            prestige 0.82\n",
      "           commander 0.82\n",
      "              caesar 0.82\n",
      "         settlements 0.82\n",
      "          blitzkrieg 0.82\n",
      "                maps 0.81\n",
      "         suitability 0.81\n",
      "               catan 0.81\n",
      "               reich 0.81\n",
      "           supremacy 0.81\n",
      "              worker 0.81\n",
      "               focus 0.81\n",
      "              russia 0.81\n",
      "              swords 0.81\n",
      "               wings 0.81\n",
      "                  vp 0.80\n",
      "              swedes 0.80\n",
      "               shogi 0.80\n",
      "                odin 0.80\n",
      "            scenario 0.80\n",
      "             austria 0.80\n",
      "           regiments 0.80\n",
      "         battlefield 0.80\n",
      "              reason 0.80\n",
      "            decisive 0.80\n",
      "                axis 0.80\n",
      "                frac 0.80\n",
      "            generals 0.80\n",
      "              bomber 0.80\n",
      "             markets 0.79\n",
      "            waterloo 0.79\n",
      "                navy 0.79\n",
      "              leader 0.79\n",
      "             council 0.79\n",
      "           logistics 0.79\n",
      "              tribes 0.79\n",
      "           formation 0.79\n",
      "           districts 0.79\n",
      "                 iii 0.79\n",
      "       countersheets 0.79\n",
      "Bad words\t     P(good | word)\n",
      "            simpsons 0.20\n",
      "              polein 0.20\n",
      "            answered 0.20\n",
      "            supplier 0.20\n",
      "              sigmaf 0.20\n",
      "              milton 0.20\n",
      "                 ate 0.19\n",
      "            marriage 0.19\n",
      "             teaches 0.19\n",
      "            dogfight 0.19\n",
      "              rabbit 0.19\n",
      "            cassette 0.19\n",
      "               berry 0.19\n",
      "              pandas 0.19\n",
      "               panda 0.19\n",
      "               chuck 0.19\n",
      "                  nu 0.19\n",
      "                 mon 0.18\n",
      "                 pig 0.18\n",
      "            children 0.18\n",
      "            leonardo 0.18\n",
      "            detector 0.18\n",
      "             bradley 0.18\n",
      "           butterfly 0.18\n",
      "                 dvd 0.18\n",
      "              garlic 0.18\n",
      "                pegs 0.17\n",
      "                bart 0.17\n",
      "              hounds 0.17\n",
      "               cones 0.17\n",
      "               beads 0.17\n",
      "                 jam 0.17\n",
      "                 boy 0.17\n",
      "              casino 0.17\n",
      "                crap 0.17\n",
      "                rent 0.17\n",
      "                 pok 0.16\n",
      "             finnish 0.15\n",
      "             spinner 0.15\n",
      "           answering 0.15\n",
      "               bible 0.14\n",
      "           questions 0.14\n",
      "                spin 0.14\n",
      "              sudoku 0.14\n",
      "              trivia 0.13\n",
      "          manoeuvres 0.12\n",
      "                mole 0.10\n",
      "             trivial 0.10\n",
      "              disney 0.08\n",
      "            monopoly 0.07\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(xtest.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:50]]\n",
    "bad_words = words[ind[-50:]]\n",
    "\n",
    "good_prob = probs[ind[:50]]\n",
    "bad_prob = probs[ind[-50:]]\n",
    "\n",
    "print(\"Good words\\t     P(good | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Bad words\\t     P(good | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
